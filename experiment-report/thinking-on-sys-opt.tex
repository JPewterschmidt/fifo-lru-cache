\section{对系统优化工作的感想}

计算机系统是一个有限的系统，对其进行优化无非就是找到各种权衡(trade off)点，
放弃一些我们不在乎的特性，来换取一些我们关注的性能。
新硬件的应用也应该在这些点上。

\subsection{新软件结构和新算法}

对软件进行性能优化，我认为大致由两个出发点：
一是从软件结构上入手，消除系统瓶颈，避免资源浪费；
二是从算法创新上入手，提高相同计算资源下的性能产出。
如果工作任务里面两者都有，我认为先解决结构性问题比较重要，
这样能够减少一些算法创新工作中的噪音。

本次考核文章\cite{qiu_frozenhot_2023}做的工作，我认为属于前者。
文中的系统管理开销会随着单机并发数量的提到而提高，是非常典型的并发场景下的问题，
一定是发生了某种 contention。
解决此类问题，我觉得唯一的大方向就是在保持并行规模的前提下，
减少线程间互相可见的操作。毕竟没有互操作就没有并发管理开销。
一方面，我想能不能把计算任务分区，让各执行单元自己完成一部分计算，事后再reduce到一起。
如早期分布式应用中的 Map-Reduce。或者线程池用的 Work-Stealing 队列。
另一方面，可以避免不必要的计算任务。算得少自然管理开销就小。

考核文章主要以优化软件结构的方法入手：
利用数据局部性特点，设置只读前端缓存来相应多数缓存请求，
从而使得多数缓存访问无需上锁，无需写入更改，以达到管理成本的目的。

综合对该文章的学习和以往的经验，我认为从软件结构层面入手对系统进行优化
首先要对计算机各通用硬件和操作系统有深入的了解，
然后针对其中一些特点总结一些一般的方法，尤其是和访存有关的技巧。
最后深入应用需要，明确权衡(trade off)点方可作出有成效的优化。

消除软件结构性问题后，就可以通过新算法进行优化。
本人将算法分为两类：一类是静态的，其行为不受数据影响(比如LRU, LFU, FIFO .etc)；
一类是动态的，其行为会反应数据特点(涉及机器学习的算法等)。
一些工作利用机器学习的手段对缓存 Eviction 进行优化，就属于后者。
GL-Cache 的工作是通过引入群组化的学习，
改变学习粒度和对象，避免了一些问题\cite{yang_gl-cache_nodate}。
是一种结合软件结构和算法的优化工作。

\subsection{新应用场景和新硬件}

有关新硬件的应用据我了解主要还是一些为特定计算或存储任务准备的特殊化硬件。
比如使用FPGA来加速学习管线中一些特殊的计算任务\cite{nurvi_2020_fpga}。
再比如存储方面，NBJL的一项工作
结合PM-NVM硬件以及LSM结构上的创新，避免了传统LSM在WAL和$L_0, L_1$层写放大问题，
对写入性能作出了显著的优化\cite{he_flatlsm_2023}。
通过新硬件加持，可以提升系统对特定问题的解决能力。

新的应用场景比如云服务、云游戏等基于云的业务，
都是在大型集群中工作的。这种类型的应用往往需要做成分布式系统，
以便利用分布式集群负载均衡、横向扩展的能力。
如\cite{qiaolin_yu_caas-lsm_2024}提出的CaaS-LSM将LSM的Compaction过程与各LSM-KVS解耦，
作为一个微服务部署在计算节点上，
这样就解决了在集群上Compaction负载不均衡的问题，提高了集群的整体利用率。

此外，无论是计算还是存储，一个大型集群的各节点地理上非常接近，
虽然软件以分布式的方式运行，但实际上计算和存储是高度中心化的。
我觉得可以利用这一特点对为特定业务做有针对性的优化，
就像多个线程可以共享数据一样，
让一些分布式的应用系统在各个节点间给不同的请求共享一些计算成果。
这样在云服务本身提供的就不仅是算力和存储的横向拓展了。

\subsection{写到最后}

纸上得来终觉浅，绝知此事要躬行。找权衡点说起来容易做起来难，
在收到本次考核要求之前我在寻找一个减少线程间互操作的方法。
思路一直局限在代码细节上，没想过像Redis这样的Sampling 方案及其
并发架构。还是实践的太少，未来无论如何要多学习业界先进方案，多实践。

另外，在最近阅读的一些文章中，一些参数化问题往往需要通过数学建模来解决。
比如Kangaro对失配率和SSD写入率的Markov建模\cite{mcallister_kangaroo_2021},
是我目前无法做得到的。因此数学表达和建模能力是非常重要的。
我个人在这方面还有非常大的进步空间。
